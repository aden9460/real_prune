nohup: 忽略输入
GPU7 第一次检测占用为 46424 MiB，等待中...
GPU7 第一次检测占用为 46424 MiB，等待中...
GPU7 第一次检测占用为 46424 MiB，等待中...
GPU7 第一次检测显存占用为 0 MiB，3 秒后再次确认...
GPU7 连续两次显存占用为 0 MiB，执行 Python 程序...
W0916 00:34:48.109871 140133748204608 torch/distributed/run.py:757] 
W0916 00:34:48.109871 140133748204608 torch/distributed/run.py:757] *****************************************
W0916 00:34:48.109871 140133748204608 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0916 00:34:48.109871 140133748204608 torch/distributed/run.py:757] *****************************************
[dist initialize] mp method=spawn
[dist initialize] mp method=spawn
[dist initialize] mp method=spawn
[dist initialize] mp method=spawn
[dist initialize] mp method=spawn
[dist initialize] mp method=spawn
[dist initialize] mp method=spawn
[dist initialize] mp method=spawn
[lrk=0, rk=0]
[lrk=6, rk=6]
[lrk=5, rk=5]
[lrk=1, rk=1]
[lrk=4, rk=4]
[lrk=3, rk=3]
[lrk=2, rk=2]
[lrk=7, rk=7]
[09-16 00:34:52] (_train/utils/arg_util.py, line 179)=> [tf32] [precis] torch.get_float32_matmul_precision(): high
[09-16 00:34:52] (_train/utils/arg_util.py, line 180)=> [tf32] [ conv ] torch.backends.cudnn.allow_tf32: True
[09-16 00:34:52] (_train/utils/arg_util.py, line 181)=> [tf32] [matmul] torch.backends.cuda.matmul.allow_tf32: True
[09-16 00:34:53] (prune/VAR_train/train.py, line  41)=> global bs=480, local bs=60
[09-16 00:34:53] (prune/VAR_train/train.py, line  42)=> initial args:
{
  data_path           : /home/suanba/datasets/ImageNet-1K
  exp_name            : text
  vfast               : 0
  vae_path            : /home/suanba/EdgeVAR/slimgpt_pub/model_zoo/model_zoo/vae_ch160v4096z32.pth
  var_path            : /home/suanba/EdgeVAR/real_prune/slimgpt_pub_prune/sparsity_model/prune_d16_0.2sparsity_15i-2_full_avegrage_scale.pth
  tfast               : 0
  depth               : 16
  sparsity            : 0.2
  ini                 : -1
  hd                  : 0.02
  aln                 : 0.5
  alng                : 0.001
  fp16                : 1
  tblr                : 0.0001
  tlr                 : 0.0001875
  twd                 : 0.05
  twde                : 0.05
  tclip               : 2.0
  ls                  : 0.0
  maxlayer            : 16
  bs                  : 480
  batch_size          : 60
  glb_batch_size      : 480
  ac                  : 1
  ep                  : 20
  wp                  : 0.4
  wp0                 : 0.005
  wpe                 : 0.1
  sche                : lin0
  opt                 : adamw
  afuse               : True
  saln                : False
  anorm               : True
  fuse                : True
  pn                  : 1_2_3_4_5_6_8_10_13_16
  patch_size          : 16
  patch_nums          : (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)
  resos               : (16, 32, 48, 64, 80, 96, 128, 160, 208, 256)
  data_load_reso      : 256
  mid_reso            : 1.125
  hflip               : False
  workers             : 0
  pg                  : 0.0
  pg0                 : 4
  pgwp                : 0.06666666666666667
  cmd                 : --depth=16 --bs=480 --ep=20 --fp16=1 --alng=1e-3 --wpe=0.1 --sparsity=0.2 --local_out_dir_path=/home/suanba/EdgeVAR/real_prune/VAR_train/0.2_d16_real_20epoch_new_average_slimgpt_15i --data_path=/home/suanba/datasets/ImageNet-1K --var_path=/home/suanba/EdgeVAR/real_prune/slimgpt_pub_prune/sparsity_model/prune_d16_0.2sparsity_15i-2_full_avegrage_scale.pth --vae_path=/home/suanba/EdgeVAR/slimgpt_pub/model_zoo/model_zoo/vae_ch160v4096z32.pth
  branch              : main
  commit_id           : 3011c066ecd17fd236eb1237cb23e132b995e187
  commit_msg          : real
  acc_mean            : None
  acc_tail            : None
  L_mean              : None
  L_tail              : None
  vacc_mean           : None
  vacc_tail           : None
  vL_mean             : None
  vL_tail             : None
  grad_norm           : None
  cur_lr              : None
  cur_wd              : None
  cur_it              : 
  cur_ep              : 
  remain_time         : 
  finish_time         : 
  local_out_dir_path  : /home/suanba/EdgeVAR/real_prune/VAR_train/0.2_d16_real_20epoch_new_average_slimgpt_15i
  tb_log_dir_path     : /home/suanba/EdgeVAR/real_prune/VAR_train/0.2_d16_real_20epoch_new_average_slimgpt_15i/tb-VARd16__pn1_2_3_4_5_6_8_10_13_16__b480ep20adamlr0.0001wd0.05
  log_txt_path        : /home/suanba/EdgeVAR/real_prune/VAR_train/0.2_d16_real_20epoch_new_average_slimgpt_15i/log.txt
  last_ckpt_path      : /home/suanba/EdgeVAR/real_prune/VAR_train/0.2_d16_real_20epoch_new_average_slimgpt_15i/ar-ckpt-last.pth
  tf32                : True
  seed                : None
  transfer            : False
  same_seed_for_all_ranks: 0
  local_debug         : False
  dbg_nan             : False
}

[09-16 00:34:53] (prune/VAR_train/train.py, line  46)=> [build PT data] ...

[09-16 00:35:47] (/VAR_train/utils/data.py, line  34)=> [Dataset] len(train_set)=1281167, len(val_set)=50000, num_classes=1000
[09-16 00:35:47] (/VAR_train/utils/data.py, line  48)=> Transform [train] = 
[09-16 00:35:47] (/VAR_train/utils/data.py, line  51)=> Resize(size=288, interpolation=lanczos, max_size=None, antialias=True)
[09-16 00:35:47] (/VAR_train/utils/data.py, line  51)=> RandomCrop(size=(256, 256), padding=None)
[09-16 00:35:47] (/VAR_train/utils/data.py, line  51)=> ToTensor()
[09-16 00:35:47] (/VAR_train/utils/data.py, line  51)=> <function normalize_01_into_pm1 at 0x7f515885a670>
[09-16 00:35:47] (/VAR_train/utils/data.py, line  54)=> ---------------------------

[09-16 00:35:47] (/VAR_train/utils/data.py, line  48)=> Transform [val] = 
[09-16 00:35:47] (/VAR_train/utils/data.py, line  51)=> Resize(size=288, interpolation=lanczos, max_size=None, antialias=True)
[09-16 00:35:47] (/VAR_train/utils/data.py, line  51)=> CenterCrop(size=(256, 256))
[09-16 00:35:47] (/VAR_train/utils/data.py, line  51)=> ToTensor()
[09-16 00:35:47] (/VAR_train/utils/data.py, line  51)=> <function normalize_01_into_pm1 at 0x7f515885a670>
[09-16 00:35:47] (/VAR_train/utils/data.py, line  54)=> ---------------------------

[09-16 00:35:47] (prune/VAR_train/train.py, line  69)=> [auto_resume] no ckpt found @ /home/suanba/EdgeVAR/real_prune/VAR_train/0.2_d16_real_20epoch_new_average_slimgpt_15i/ar-ckpt*.pth
[09-16 00:35:47] (prune/VAR_train/train.py, line  69)=> [auto_resume quit]
[09-16 00:35:47] (prune/VAR_train/train.py, line  70)=> [dataloader multi processing] ...     [dataloader multi processing](*) finished! (0.00s)
[09-16 00:35:47] (prune/VAR_train/train.py, line  76)=> [dataloader] gbs=480, lbs=60, iters_train=2670, types(tr, va)=('DatasetFolder', 'DatasetFolder')
[09-16 00:35:48] (/VAR_train/models/var.py, line 102)=> 
[constructor]  ==== flash_if_available=True (0/16), fused_if_available=True (fusing_add_ln=0/16, fusing_mlp=0/16) ==== 
    [VAR config ] embed_dim=1024, num_heads=16, depth=16, mlp_ratio=4.0
    [drop ratios ] drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0666667 (tensor([0.0000, 0.0044, 0.0089, 0.0133, 0.0178, 0.0222, 0.0267, 0.0311, 0.0356,
        0.0400, 0.0444, 0.0489, 0.0533, 0.0578, 0.0622, 0.0667]))

[09-16 00:35:48] (/VAR_train/models/var.py, line 259)=> [init_weights] VAR with init_std=0.0180422
[09-16 00:35:51] (prune/VAR_train/train.py, line 108)=> 加载原始模型权重...
[09-16 00:35:53] (prune/VAR_train/train.py, line 124)=> [INIT] VAR model = VAR(
  drop_path_rate=0.0666667
  (word_embed): Linear(in_features=32, out_features=1024, bias=True)
  (class_emb): Embedding(1001, 1024)
  (lvl_embed): Embedding(10, 1024)
  (shared_ada_lin): Identity()
  (blocks): ModuleList(
    (0): AdaLNSelfAttn(
      shared_aln=False
      (drop_path): Identity()
      (attn): SelfAttention(
        using_flash=False, using_xform=False, attn_l2_norm=True
        (mat_qkv): Linear(in_features=1024, out_features=2496, bias=False)
        (proj): Linear(in_features=832, out_features=1024, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp_func=False
        (fc1): Linear(in_features=1024, out_features=3277, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=3277, out_features=1024, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
      (ada_lin): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1024, out_features=6144, bias=True)
      )
    )
    (1-15): 15 x AdaLNSelfAttn(
      shared_aln=False
      (drop_path): DropPath((drop_prob=...))
      (attn): SelfAttention(
        using_flash=False, using_xform=False, attn_l2_norm=True
        (mat_qkv): Linear(in_features=1024, out_features=2496, bias=False)
        (proj): Linear(in_features=832, out_features=1024, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp_func=False
        (fc1): Linear(in_features=1024, out_features=3277, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=3277, out_features=1024, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
      (ada_lin): Sequential(
        (0): SiLU()
        (1): Linear(in_features=1024, out_features=6144, bias=True)
      )
    )
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=1024, out_features=2048, bias=True)
    )
  )
  (head): Linear(in_features=1024, out_features=4096, bias=True)
)


[09-16 00:35:53] (prune/VAR_train/train.py, line 126)=> [INIT][#para] VAE=108.95, VAE.enc=44.11, VAE.dec=64.65, VAE.quant=0.17
[09-16 00:35:53] (prune/VAR_train/train.py, line 127)=> [INIT][#para] VAR=270.84


[09-16 00:35:53] (rain/utils/lr_control.py, line  99)=> [get_param_groups] param_groups = 
{ 'D': { 'lr_sc': 1.0,
         'params': "('word_embed.weight, class_emb.weight, blocks.0.attn.mat_qkv.weight, blocks.0.attn.proj.weight, blocks.0.ffn.fc1.weight, blocks.0.ffn.fc2.weight, blocks.0.ada_lin.1.weight, '\n"
                   " 'blocks.1.attn.mat_qkv.weight, blocks.1.attn.proj.weight, blocks.1.ffn.fc1.weight, blocks.1.ffn.fc2.weight, blocks.1.ada_lin.1.weight, blocks.2.attn.mat_qkv.weight, blocks.2.attn.proj.weight, '\n"
                   " 'blocks.2.ffn.fc1.weight, blocks.2.ffn.fc2.weight, blocks.2.ada_lin.1.weight, blocks.3.attn.mat_qkv.weight, blocks.3.attn.proj.weight, blocks.3.ffn.fc1.weight, blocks.3.ffn.fc2.weight, '\n"
                   " 'blocks.3.ada_lin.1.weight, blocks.4.attn.mat_qkv.weight, blocks.4.attn.proj.weight, blocks.4.ffn.fc1.weight, blocks.4.ffn.fc2.weight, blocks.4.ada_lin.1.weight, blocks.5.attn.mat_qkv.weight, '\n"
                   " 'blocks.5.attn.proj.weight, blocks.5.ffn.fc1.weight, blocks.5.ffn.fc2.weight, blocks.5.ada_lin.1.weight, blocks.6.attn.mat_qkv.weight, blocks.6.attn.proj.weight, blocks.6.ffn.fc1.weight, '\n"
                   " 'blocks.6.ffn.fc2.weight, blocks.6.ada_lin.1.weight, blocks.7.attn.mat_qkv.weight, blocks.7.attn.proj.weight, blocks.7.ffn.fc1.weight, blocks.7.ffn.fc2.weight, blocks.7.ada_lin.1.weight, '\n"
                   " 'blocks.8.attn.mat_qkv.weight, blocks.8.attn.proj.weight, blocks.8.ffn.fc1.weight, blocks.8.ffn.fc2.weight, blocks.8.ada_lin.1.weight, blocks.9.attn.mat_qkv.weight, blocks.9.attn.proj.weight, '\n"
                   " 'blocks.9.ffn.fc1.weight, blocks.9.ffn.fc2.weight, blocks.9.ada_lin.1.weight, blocks.10.attn.mat_qkv.weight, blocks.10.attn.proj.weight, blocks.10.ffn.fc1.weight, blocks.10.ffn.fc2.weight, '\n"
                   " 'blocks.10.ada_lin.1.weight, blocks.11.attn.mat_qkv.weight, blocks.11.attn.proj.weight, blocks.11.ffn.fc1.weight, blocks.11.ffn.fc2.weight, blocks.11.ada_lin.1.weight, '\n"
                   " 'blocks.12.attn.mat_qkv.weight, blocks.12.attn.proj.weight, blocks.12.ffn.fc1.weight, blocks.12.ffn.fc2.weight, blocks.12.ada_lin.1.weight, blocks.13.attn.mat_qkv.weight, '\n"
                   " 'blocks.13.attn.proj.weight, blocks.13.ffn.fc1.weight, blocks.13.ffn.fc2.weight, blocks.13.ada_lin.1.weight, blocks.14.attn.mat_qkv.weight, blocks.14.attn.proj.weight, blocks.14.ffn.fc1.weight, '\n"
                   " 'blocks.14.ffn.fc2.weight, blocks.14.ada_lin.1.weight, blocks.15.attn.mat_qkv.weight, blocks.15.attn.proj.weight, blocks.15.ffn.fc1.weight, blocks.15.ffn.fc2.weight, blocks.15.ada_lin.1.weight, '\n"
                   " 'head_nm.ada_lin.1.weight, head.weight')",
         'wd_sc': 1.0},
  'ND': { 'lr_sc': 1.0,
          'params': "('pos_start, pos_1LC, word_embed.bias, lvl_embed.weight, blocks.0.attn.scale_mul_1H11, blocks.0.attn.q_bias, blocks.0.attn.v_bias, blocks.0.attn.proj.bias, blocks.0.ffn.fc1.bias, '\n"
                    " 'blocks.0.ffn.fc2.bias, blocks.0.ada_lin.1.bias, blocks.1.attn.scale_mul_1H11, blocks.1.attn.q_bias, blocks.1.attn.v_bias, blocks.1.attn.proj.bias, blocks.1.ffn.fc1.bias, blocks.1.ffn.fc2.bias, '\n"
                    " 'blocks.1.ada_lin.1.bias, blocks.2.attn.scale_mul_1H11, blocks.2.attn.q_bias, blocks.2.attn.v_bias, blocks.2.attn.proj.bias, blocks.2.ffn.fc1.bias, blocks.2.ffn.fc2.bias, blocks.2.ada_lin.1.bias, '\n"
                    " 'blocks.3.attn.scale_mul_1H11, blocks.3.attn.q_bias, blocks.3.attn.v_bias, blocks.3.attn.proj.bias, blocks.3.ffn.fc1.bias, blocks.3.ffn.fc2.bias, blocks.3.ada_lin.1.bias, '\n"
                    " 'blocks.4.attn.scale_mul_1H11, blocks.4.attn.q_bias, blocks.4.attn.v_bias, blocks.4.attn.proj.bias, blocks.4.ffn.fc1.bias, blocks.4.ffn.fc2.bias, blocks.4.ada_lin.1.bias, '\n"
                    " 'blocks.5.attn.scale_mul_1H11, blocks.5.attn.q_bias, blocks.5.attn.v_bias, blocks.5.attn.proj.bias, blocks.5.ffn.fc1.bias, blocks.5.ffn.fc2.bias, blocks.5.ada_lin.1.bias, '\n"
                    " 'blocks.6.attn.scale_mul_1H11, blocks.6.attn.q_bias, blocks.6.attn.v_bias, blocks.6.attn.proj.bias, blocks.6.ffn.fc1.bias, blocks.6.ffn.fc2.bias, blocks.6.ada_lin.1.bias, '\n"
                    " 'blocks.7.attn.scale_mul_1H11, blocks.7.attn.q_bias, blocks.7.attn.v_bias, blocks.7.attn.proj.bias, blocks.7.ffn.fc1.bias, blocks.7.ffn.fc2.bias, blocks.7.ada_lin.1.bias, '\n"
                    " 'blocks.8.attn.scale_mul_1H11, blocks.8.attn.q_bias, blocks.8.attn.v_bias, blocks.8.attn.proj.bias, blocks.8.ffn.fc1.bias, blocks.8.ffn.fc2.bias, blocks.8.ada_lin.1.bias, '\n"
                    " 'blocks.9.attn.scale_mul_1H11, blocks.9.attn.q_bias, blocks.9.attn.v_bias, blocks.9.attn.proj.bias, blocks.9.ffn.fc1.bias, blocks.9.ffn.fc2.bias, blocks.9.ada_lin.1.bias, '\n"
                    " 'blocks.10.attn.scale_mul_1H11, blocks.10.attn.q_bias, blocks.10.attn.v_bias, blocks.10.attn.proj.bias, blocks.10.ffn.fc1.bias, blocks.10.ffn.fc2.bias, blocks.10.ada_lin.1.bias, '\n"
                    " 'blocks.11.attn.scale_mul_1H11, blocks.11.attn.q_bias, blocks.11.attn.v_bias, blocks.11.attn.proj.bias, blocks.11.ffn.fc1.bias, blocks.11.ffn.fc2.bias, blocks.11.ada_lin.1.bias, '\n"
                    " 'blocks.12.attn.scale_mul_1H11, blocks.12.attn.q_bias, blocks.12.attn.v_bias, blocks.12.attn.proj.bias, blocks.12.ffn.fc1.bias, blocks.12.ffn.fc2.bias, blocks.12.ada_lin.1.bias, '\n"
                    " 'blocks.13.attn.scale_mul_1H11, blocks.13.attn.q_bias, blocks.13.attn.v_bias, blocks.13.attn.proj.bias, blocks.13.ffn.fc1.bias, blocks.13.ffn.fc2.bias, blocks.13.ada_lin.1.bias, '\n"
                    " 'blocks.14.attn.scale_mul_1H11, blocks.14.attn.q_bias, blocks.14.attn.v_bias, blocks.14.attn.proj.bias, blocks.14.ffn.fc1.bias, blocks.14.ffn.fc2.bias, blocks.14.ada_lin.1.bias, '\n"
                    " 'blocks.15.attn.scale_mul_1H11, blocks.15.attn.q_bias, blocks.15.attn.v_bias, blocks.15.attn.proj.bias, blocks.15.ffn.fc1.bias, blocks.15.ffn.fc2.bias, blocks.15.ada_lin.1.bias, '\n"
                    " 'head_nm.ada_lin.1.bias, head.bias')",
          'wd_sc': 0.0}}

[09-16 00:35:53] (rain/utils/lr_control.py, line 104)=> [get_param_groups][rank0] type(model).__name__='VAR' count=202, numel=270844320
[09-16 00:35:53] (rain/utils/lr_control.py, line 104)=> [get_param_groups][rank1] type(model).__name__='VAR' count=202, numel=270844320
[09-16 00:35:53] (rain/utils/lr_control.py, line 104)=> [get_param_groups][rank2] type(model).__name__='VAR' count=202, numel=270844320
[09-16 00:35:53] (rain/utils/lr_control.py, line 104)=> [get_param_groups][rank3] type(model).__name__='VAR' count=202, numel=270844320
[09-16 00:35:53] (rain/utils/lr_control.py, line 104)=> [get_param_groups][rank4] type(model).__name__='VAR' count=202, numel=270844320
[09-16 00:35:53] (rain/utils/lr_control.py, line 104)=> [get_param_groups][rank5] type(model).__name__='VAR' count=202, numel=270844320
[09-16 00:35:53] (rain/utils/lr_control.py, line 104)=> [get_param_groups][rank6] type(model).__name__='VAR' count=202, numel=270844320
[09-16 00:35:53] (rain/utils/lr_control.py, line 105)=> 
[09-16 00:35:53] (prune/VAR_train/train.py, line 142)=> [INIT] optim=functools.partial(<class 'torch.optim.adamw.AdamW'>, betas=(0.9, 0.95), fused=True), opt_kw={'lr': 0.0001875, 'weight_decay': 0}

[09-16 00:35:53] (rain/utils/lr_control.py, line 104)=> [get_param_groups][rank7] type(model).__name__='VAR' count=202, numel=270844320
[09-16 00:35:58] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   0/20]  [   0/2670]  eta: 3:31:51  tlr: 9.4e-07  tnm: 6.53  Lm: 8.455 (8.455)  Lt: 8.256 (8.256)  Accm: 0.10 (0.10)  Acct: 0.07 (0.07)  time: 4.7610  data: 0.9323
[09-16 01:06:36] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   0/20]  [ 667/2670]  eta: 1:32:04  tlr: 0.00012  tnm: 1.47  Lm: 7.683 (7.683)  Lt: 7.483 (7.483)  Accm: 1.05 (1.05)  Acct: 1.15 (1.15)  time: 3.1772  data: 1.8226
[09-16 01:36:57] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   0/20]  [1334/2670]  eta: 1:01:06  tlr: 0.00019  tnm: 0.61  Lm: 6.912 (7.310)  Lt: 6.710 (7.034)  Accm: 1.99 (1.70)  Acct: 2.23 (2.08)  time: 2.0513  data: 0.7632
[09-16 02:07:44] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   0/20]  [2001/2670]  eta: 0:30:41  tlr: 0.00019  tnm: 0.46  Lm: 6.737 (7.074)  Lt: 6.424 (6.725)  Accm: 2.50 (2.26)  Acct: 3.09 (2.94)  time: 3.3261  data: 1.7898
[09-16 02:38:00] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   0/20]  [2669/2670]  eta: 0:00:02  tlr: 0.00019  tnm: 0.43  Lm: 6.563 (6.927)  Lt: 6.137 (6.534)  Accm: 3.01 (2.61)  Acct: 3.95 (3.50)  time: 2.2042  data: 0.8787
[09-16 02:38:00] (/VAR_train/utils/misc.py, line 336)=> [Ep]: [   0/20]   Total time:      2:02:06   (2.744 s / it)
[09-16 02:39:49] (prune/VAR_train/train.py, line 236)=>  [*] [ep0]  (val 50000)  Lm: 6.9290, Lt: 6.5318, Acc m&t: 2.59 3.54,  Val cost: 109.78s
[09-16 02:39:49] (prune/VAR_train/train.py, line 241)=> [saving ckpt] ...     [saving ckpt](*) finished!  @ /home/suanba/EdgeVAR/real_prune/VAR_train/0.2_d16_real_20epoch_new_average_slimgpt_15i/ar-ckpt-last.pth
[09-16 02:39:54] (prune/VAR_train/train.py, line 253)=>      [ep0]  (training )  Lm: 6.929 (6.929), Lt: 6.532 (6.532),  Acc m&t: 2.59 3.54,  Remain: 1 day, 4:01:42,  Finish: 2025-09-17 06:39
[09-16 02:39:56] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   1/20]  [   0/2670]  eta: 1:22:32  tlr: 0.00019  tnm: 0.44  Lm: 6.323 (6.323)  Lt: 5.692 (5.692)  Accm: 3.87 (3.87)  Acct: 5.87 (5.87)  time: 1.8548  data: 0.6269
[09-16 03:10:40] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   1/20]  [ 667/2670]  eta: 1:32:14  tlr: 0.00019  tnm: 0.39  Lm: 6.283 (6.283)  Lt: 5.662 (5.662)  Accm: 4.03 (4.03)  Acct: 5.97 (5.97)  time: 2.7739  data: 1.4655
[09-16 03:41:05] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   1/20]  [1334/2670]  eta: 1:01:13  tlr: 0.00019  tnm: 0.35  Lm: 6.323 (6.303)  Lt: 5.690 (5.671)  Accm: 3.90 (3.99)  Acct: 5.94 (5.96)  time: 3.3049  data: 1.8472
[09-16 04:11:53] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   1/20]  [2001/2670]  eta: 0:30:44  tlr: 0.00018  tnm: 0.38  Lm: 6.283 (6.280)  Lt: 5.661 (5.652)  Accm: 4.05 (4.19)  Acct: 6.00 (6.15)  time: 2.4167  data: 1.0673
[09-16 04:42:26] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   1/20]  [2669/2670]  eta: 0:00:02  tlr: 0.00018  tnm: 0.33  Lm: 6.314 (6.287)  Lt: 5.690 (5.669)  Accm: 3.90 (4.13)  Acct: 5.94 (6.07)  time: 2.2860  data: 0.9468
[09-16 04:42:26] (/VAR_train/utils/misc.py, line 336)=> [Ep]: [   1/20]   Total time:      2:02:31   (2.753 s / it)
[09-16 04:44:08] (prune/VAR_train/train.py, line 236)=>  [*] [ep1]  (val 50000)  Lm: 6.2891, Lt: 5.6610, Acc m&t: 4.12 6.16,  Val cost: 102.07s
[09-16 04:44:08] (prune/VAR_train/train.py, line 241)=> [saving ckpt] ...     [saving ckpt](*) finished!  @ /home/suanba/EdgeVAR/real_prune/VAR_train/0.2_d16_real_20epoch_new_average_slimgpt_15i/ar-ckpt-last.pth
[09-16 04:44:12] (prune/VAR_train/train.py, line 253)=>      [ep1]  (training )  Lm: 6.289 (6.289), Lt: 5.661 (5.661),  Acc m&t: 4.12 6.16,  Remain: 1 day, 2:57:45,  Finish: 2025-09-17 07:40
[09-16 04:44:16] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   2/20]  [   0/2670]  eta: 2:46:53  tlr: 0.00018  tnm: 0.35  Lm: 6.223 (6.223)  Lt: 5.513 (5.513)  Accm: 4.51 (4.51)  Acct: 6.98 (6.98)  time: 3.7503  data: 2.5369
[09-16 05:15:12] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   2/20]  [ 667/2670]  eta: 1:32:56  tlr: 0.00018  tnm: 0.34  Lm: 6.246 (6.246)  Lt: 5.559 (5.559)  Accm: 4.35 (4.35)  Acct: 6.65 (6.65)  time: 3.2287  data: 1.7455
[09-16 05:45:38] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   2/20]  [1334/2670]  eta: 1:01:28  tlr: 0.00018  tnm: 0.32  Lm: 6.268 (6.258)  Lt: 5.606 (5.590)  Accm: 4.18 (4.27)  Acct: 6.32 (6.53)  time: 2.1293  data: 0.7985
[09-16 06:16:36] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   2/20]  [2001/2670]  eta: 0:30:52  tlr: 0.00018  tnm: 0.32  Lm: 6.270 (6.262)  Lt: 5.628 (5.610)  Accm: 4.16 (4.24)  Acct: 6.31 (6.44)  time: 3.2267  data: 1.7772
[09-16 06:46:58] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   2/20]  [2669/2670]  eta: 0:00:02  tlr: 0.00017  tnm: 0.32  Lm: 6.268 (6.249)  Lt: 5.606 (5.599)  Accm: 4.18 (4.30)  Acct: 6.32 (6.43)  time: 2.1136  data: 0.7895
[09-16 06:46:58] (/VAR_train/utils/misc.py, line 336)=> [Ep]: [   2/20]   Total time:      2:02:46   (2.759 s / it)
[09-16 06:48:43] (prune/VAR_train/train.py, line 236)=>  [*] [ep2]  (val 50000)  Lm: 6.2371, Lt: 5.5896, Acc m&t: 4.35 6.53,  Val cost: 104.84s
[09-16 06:48:43] (prune/VAR_train/train.py, line 241)=> [saving ckpt] ...     [saving ckpt](*) finished!  @ /home/suanba/EdgeVAR/real_prune/VAR_train/0.2_d16_real_20epoch_new_average_slimgpt_15i/ar-ckpt-last.pth
[09-16 06:48:48] (prune/VAR_train/train.py, line 253)=>      [ep2]  (training )  Lm: 6.237 (6.237), Lt: 5.590 (5.590),  Acc m&t: 4.35 6.53,  Remain: 1 day, 0:28:16,  Finish: 2025-09-17 07:15
[09-16 06:48:49] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   3/20]  [   0/2670]  eta: 1:24:31  tlr: 0.00017  tnm: 0.32  Lm: 6.243 (6.243)  Lt: 5.558 (5.558)  Accm: 4.12 (4.12)  Acct: 6.59 (6.59)  time: 1.8995  data: 0.6422
[09-16 07:19:42] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   3/20]  [ 667/2670]  eta: 1:32:39  tlr: 0.00017  tnm: 0.32  Lm: 6.248 (6.248)  Lt: 5.584 (5.584)  Accm: 4.37 (4.37)  Acct: 6.81 (6.81)  time: 2.6450  data: 1.3000
[09-16 07:50:36] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   3/20]  [1334/2670]  eta: 1:01:51  tlr: 0.00017  tnm: 0.33  Lm: 6.243 (6.246)  Lt: 5.569 (5.579)  Accm: 4.17 (4.30)  Acct: 6.59 (6.70)  time: 3.3496  data: 1.9434
[09-16 08:21:18] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   3/20]  [2001/2670]  eta: 0:30:54  tlr: 0.00017  tnm: 0.31  Lm: 6.241 (6.225)  Lt: 5.564 (5.555)  Accm: 4.39 (4.39)  Acct: 6.81 (6.81)  time: 2.0277  data: 0.6879
[09-16 08:53:05] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   3/20]  [2669/2670]  eta: 0:00:02  tlr: 0.00016  tnm: 0.32  Lm: 6.240 (6.196)  Lt: 5.558 (5.531)  Accm: 4.62 (4.53)  Acct: 7.04 (6.98)  time: 3.3225  data: 1.8733
[09-16 08:53:05] (/VAR_train/utils/misc.py, line 336)=> [Ep]: [   3/20]   Total time:      2:04:17   (2.793 s / it)
[09-16 08:54:41] (prune/VAR_train/train.py, line 236)=>  [*] [ep3]  (val 50000)  Lm: 6.2346, Lt: 5.5798, Acc m&t: 4.30 6.51,  Val cost: 96.76s
[09-16 08:54:41] (prune/VAR_train/train.py, line 241)=> [saving ckpt] ...     [saving ckpt](*) finished!  @ /home/suanba/EdgeVAR/real_prune/VAR_train/0.2_d16_real_20epoch_new_average_slimgpt_15i/ar-ckpt-last.pth
[09-16 08:54:46] (prune/VAR_train/train.py, line 253)=>      [ep3]  (training )  Lm: 6.235 (6.235), Lt: 5.580 (5.580),  Acc m&t: 4.35 6.53,  Remain: 1 day, 15:57:51,  Finish: 2025-09-18 00:50
[09-16 08:54:49] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   4/20]  [   0/2670]  eta: 2:17:03  tlr: 0.00016  tnm: 0.30  Lm: 6.269 (6.269)  Lt: 5.627 (5.627)  Accm: 4.09 (4.09)  Acct: 6.26 (6.26)  time: 3.0801  data: 1.7037
[09-16 09:26:12] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   4/20]  [ 667/2670]  eta: 1:34:15  tlr: 0.00016  tnm: 0.30  Lm: 6.246 (6.246)  Lt: 5.612 (5.612)  Accm: 4.19 (4.19)  Acct: 6.48 (6.48)  time: 2.3368  data: 0.9838
[09-16 09:57:54] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   4/20]  [1334/2670]  eta: 1:03:11  tlr: 0.00016  tnm: 0.31  Lm: 6.269 (6.278)  Lt: 5.627 (5.654)  Accm: 4.09 (4.10)  Acct: 6.26 (6.36)  time: 3.3540  data: 1.9501
[09-16 10:28:20] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   4/20]  [2001/2670]  eta: 0:31:15  tlr: 0.00016  tnm: 0.31  Lm: 6.267 (6.275)  Lt: 5.612 (5.636)  Accm: 4.08 (4.10)  Acct: 6.24 (6.32)  time: 2.1675  data: 0.8114
[09-16 10:59:24] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   4/20]  [2669/2670]  eta: 0:00:02  tlr: 0.00015  tnm: 0.31  Lm: 6.266 (6.255)  Lt: 5.597 (5.619)  Accm: 4.09 (4.22)  Acct: 6.26 (6.45)  time: 2.8131  data: 1.4709
[09-16 10:59:24] (/VAR_train/utils/misc.py, line 336)=> [Ep]: [   4/20]   Total time:      2:04:37   (2.801 s / it)
[09-16 11:01:06] (prune/VAR_train/train.py, line 236)=>  [*] [ep4]  (val 50000)  Lm: 6.2287, Lt: 5.5759, Acc m&t: 4.32 6.53,  Val cost: 102.41s
[09-16 11:01:06] (prune/VAR_train/train.py, line 241)=> [saving ckpt] ...     [saving ckpt](*) finished!  @ /home/suanba/EdgeVAR/real_prune/VAR_train/0.2_d16_real_20epoch_new_average_slimgpt_15i/ar-ckpt-last.pth
[09-16 11:01:11] (prune/VAR_train/train.py, line 253)=>      [ep4]  (training )  Lm: 6.229 (6.229), Lt: 5.576 (5.576),  Acc m&t: 4.35 6.53,  Remain: 1 day, 0:16:25,  Finish: 2025-09-17 11:15
[09-16 11:01:16] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   5/20]  [   0/2670]  eta: 3:38:25  tlr: 0.00015  tnm: 0.32  Lm: 6.223 (6.223)  Lt: 5.544 (5.544)  Accm: 4.32 (4.32)  Acct: 6.73 (6.73)  time: 4.9085  data: 3.7216
[09-16 11:32:14] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   5/20]  [ 667/2670]  eta: 1:33:07  tlr: 0.00015  tnm: 0.31  Lm: 6.205 (6.205)  Lt: 5.537 (5.537)  Accm: 4.44 (4.44)  Acct: 6.88 (6.88)  time: 3.2439  data: 1.8165
[09-16 12:02:45] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   5/20]  [1334/2670]  eta: 1:01:36  tlr: 0.00015  tnm: 0.31  Lm: 6.187 (6.190)  Lt: 5.530 (5.520)  Accm: 4.57 (4.52)  Acct: 7.02 (6.95)  time: 2.7784  data: 1.3870
[09-16 12:33:24] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   5/20]  [2001/2670]  eta: 0:30:49  tlr: 0.00015  tnm: 0.31  Lm: 6.173 (6.177)  Lt: 5.508 (5.508)  Accm: 4.62 (4.56)  Acct: 7.06 (6.99)  time: 2.4369  data: 1.0671
[09-16 13:04:33] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   5/20]  [2669/2670]  eta: 0:00:02  tlr: 0.00015  tnm: 0.30  Lm: 6.187 (6.183)  Lt: 5.486 (5.498)  Accm: 4.57 (4.53)  Acct: 7.05 (7.00)  time: 3.2203  data: 1.8344
[09-16 13:04:33] (/VAR_train/utils/misc.py, line 336)=> [Ep]: [   5/20]   Total time:      2:03:22   (2.772 s / it)
[09-16 13:06:04] (prune/VAR_train/train.py, line 236)=>  [*] [ep5]  (val 50000)  Lm: 6.2118, Lt: 5.5499, Acc m&t: 4.44 6.73,  Val cost: 90.42s
[09-16 13:06:04] (prune/VAR_train/train.py, line 241)=> [saving ckpt] ...     [saving ckpt](*) finished!  @ /home/suanba/EdgeVAR/real_prune/VAR_train/0.2_d16_real_20epoch_new_average_slimgpt_15i/ar-ckpt-last.pth
[09-16 13:06:08] (prune/VAR_train/train.py, line 253)=>      [ep5]  (training )  Lm: 6.212 (6.212), Lt: 5.550 (5.550),  Acc m&t: 4.44 6.73,  Remain: 1 day, 8:30:22,  Finish: 2025-09-17 21:34
[09-16 13:06:13] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   6/20]  [   0/2670]  eta: 3:50:39  tlr: 0.00015  tnm: 0.31  Lm: 6.277 (6.277)  Lt: 5.539 (5.539)  Accm: 4.14 (4.14)  Acct: 6.48 (6.48)  time: 5.1833  data: 3.8564
[09-16 13:36:52] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   6/20]  [ 667/2670]  eta: 1:32:10  tlr: 0.00014  tnm: 0.31  Lm: 6.251 (6.251)  Lt: 5.511 (5.511)  Accm: 4.20 (4.20)  Acct: 6.66 (6.66)  time: 3.0830  data: 1.6960
[09-16 14:07:27] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   6/20]  [1334/2670]  eta: 1:01:21  tlr: 0.00014  tnm: 0.30  Lm: 6.225 (6.199)  Lt: 5.484 (5.487)  Accm: 4.25 (4.45)  Acct: 6.84 (6.88)  time: 2.0265  data: 0.7215
[09-16 14:39:40] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   6/20]  [2001/2670]  eta: 0:31:15  tlr: 0.00014  tnm: 0.31  Lm: 6.185 (6.185)  Lt: 5.497 (5.493)  Accm: 4.60 (4.61)  Acct: 7.07 (6.98)  time: 3.3901  data: 1.9869
[09-16 15:11:55] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   6/20]  [2669/2670]  eta: 0:00:02  tlr: 0.00014  tnm: 0.32  Lm: 6.225 (6.201)  Lt: 5.510 (5.502)  Accm: 4.49 (4.58)  Acct: 7.01 (6.99)  time: 2.4706  data: 1.1193
[09-16 15:11:55] (/VAR_train/utils/misc.py, line 336)=> [Ep]: [   6/20]   Total time:      2:05:46   (2.826 s / it)
[09-16 15:13:39] (prune/VAR_train/train.py, line 236)=>  [*] [ep6]  (val 50000)  Lm: 6.1987, Lt: 5.5391, Acc m&t: 4.49 6.77,  Val cost: 104.07s
[09-16 15:13:39] (prune/VAR_train/train.py, line 241)=> [saving ckpt] ...     [saving ckpt](*) finished!  @ /home/suanba/EdgeVAR/real_prune/VAR_train/0.2_d16_real_20epoch_new_average_slimgpt_15i/ar-ckpt-last.pth
[09-16 15:13:43] (prune/VAR_train/train.py, line 253)=>      [ep6]  (training )  Lm: 6.199 (6.199), Lt: 5.539 (5.539),  Acc m&t: 4.49 6.77,  Remain: 20:04:49,  Finish: 2025-09-17 11:16
[09-16 15:13:48] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   7/20]  [   0/2670]  eta: 3:13:59  tlr: 0.00014  tnm: 0.31  Lm: 6.253 (6.253)  Lt: 5.565 (5.565)  Accm: 4.25 (4.25)  Acct: 6.48 (6.48)  time: 4.3595  data: 3.0003
[09-16 15:45:59] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   7/20]  [ 667/2670]  eta: 1:36:44  tlr: 0.00013  tnm: 0.33  Lm: 6.182 (6.182)  Lt: 5.514 (5.514)  Accm: 4.71 (4.71)  Acct: 6.94 (6.94)  time: 3.3153  data: 1.8801
[09-16 16:18:12] (/VAR_train/utils/misc.py, line 314)=> [Ep]: [   7/20]  [1334/2670]  eta: 1:04:31  tlr: 0.00013  tnm: 0.32  Lm: 6.242 (6.202)  Lt: 5.537 (5.522)  Accm: 4.44 (4.62)  Acct: 6.85 (6.91)  time: 2.8047  data: 1.3449
W0916 16:20:00.374660 140133748204608 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 889051 closing signal SIGTERM
W0916 16:20:00.381984 140133748204608 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 889052 closing signal SIGTERM
W0916 16:20:00.382238 140133748204608 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 889054 closing signal SIGTERM
W0916 16:20:00.382452 140133748204608 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 889055 closing signal SIGTERM
W0916 16:20:00.382712 140133748204608 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 889056 closing signal SIGTERM
W0916 16:20:00.382899 140133748204608 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 889057 closing signal SIGTERM
W0916 16:20:00.383101 140133748204608 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 889058 closing signal SIGTERM
E0916 16:20:01.715356 140133748204608 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: -9) local_rank: 2 (pid: 889053) of binary: /root/miniconda3/envs/common/bin/python3.9
Traceback (most recent call last):
  File "/root/miniconda3/envs/common/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/root/miniconda3/envs/common/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/root/miniconda3/envs/common/lib/python3.9/site-packages/torch/distributed/run.py", line 879, in main
    run(args)
  File "/root/miniconda3/envs/common/lib/python3.9/site-packages/torch/distributed/run.py", line 870, in run
    elastic_launch(
  File "/root/miniconda3/envs/common/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/root/miniconda3/envs/common/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 263, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
=======================================================
train.py FAILED
-------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
-------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-09-16_16:20:00
  host      : suanba
  rank      : 2 (local_rank: 2)
  exitcode  : -9 (pid: 889053)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 889053
=======================================================
